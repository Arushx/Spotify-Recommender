{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Spotify Song Recommender Model\n",
                "\n",
                "This notebook prepares the data and trains a TensorFlow classification model to predict song popularity. We will use this model in our web application to filter for high-quality songs before recommending them based on user preferences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "import joblib\n",
                "\n",
                "# 1. Load Data\n",
                "df = pd.read_csv('Popular_Spotify_Songs.csv', encoding='latin-1')\n",
                "\n",
                "# 2. Data Cleaning\n",
                "# Convert streams to numeric, coercing errors to NaN\n",
                "df['streams'] = pd.to_numeric(df['streams'], errors='coerce')\n",
                "\n",
                "# Drop rows with missing values\n",
                "df = df.dropna()\n",
                "\n",
                "# Create Target Variable (Popularity Class)\n",
                "# Low: < 150M, Medium: 150M-675M, High: > 675M\n",
                "def classify_streams(streams):\n",
                "    if streams < 150000000:\n",
                "        return 0 # Low\n",
                "    elif streams < 675000000:\n",
                "        return 1 # Medium\n",
                "    else:\n",
                "        return 2 # High\n",
                "\n",
                "df['popularity'] = df['streams'].apply(classify_streams)\n",
                "\n",
                "# Select Features for the Model\n",
                "feature_cols = [\n",
                "    'bpm', 'danceability_%', 'valence_%', 'energy_%', \n",
                "    'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%'\n",
                "]\n",
                "\n",
                "X = df[feature_cols]\n",
                "y = df['popularity']\n",
                "\n",
                "# 3. Preprocessing\n",
                "# Scale the features so they are all on the same scale (0-1 or similar)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# Split into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# 4. Build TensorFlow Model\n",
                "model = tf.keras.Sequential([\n",
                "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
                "    tf.keras.layers.Dropout(0.2),\n",
                "    tf.keras.layers.Dense(32, activation='relu'),\n",
                "    tf.keras.layers.Dense(3, activation='softmax') # 3 classes: Low, Medium, High\n",
                "])\n",
                "\n",
                "model.compile(\n",
                "    optimizer='adam',\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "# 5. Train Model\n",
                "history = model.fit(\n",
                "    X_train, y_train,\n",
                "    epochs=50,\n",
                "    batch_size=32,\n",
                "    validation_split=0.2,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Evaluate\n",
                "loss, accuracy = model.evaluate(X_test, y_test)\n",
                "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
                "\n",
                "# 6. Save Artifacts for Web App\n",
                "# Save the model\n",
                "model.save('spotify_model.keras')\n",
                "\n",
                "# Save the scaler (to scale user input later)\n",
                "joblib.dump(scaler, 'scaler.pkl')\n",
                "\n",
                "# Save the processed dataframe (with track names and artists) for recommendations\n",
                "# We need the original data + the scaled features to calculate distance\n",
                "final_df = df.copy()\n",
                "final_df[feature_cols] = scaler.transform(df[feature_cols]) # Replace features with scaled versions\n",
                "final_df.to_csv('processed_spotify_data.csv', index=False)\n",
                "\n",
                "print(\"Model and data saved successfully!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}